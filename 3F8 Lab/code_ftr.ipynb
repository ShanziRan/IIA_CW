{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('x.txt')\n",
    "y = np.loadtxt('y.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are borrowed from the jupyter notebook provided for the 3F8 short lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic function\n",
    "\n",
    "def logistic(x): return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Function that expands a matrix of input features by adding a column equal to 1.\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X: matrix of input features.\n",
    "#\n",
    "# Output: Matrix x_tilde with one additional constant column equal to 1 added.\n",
    "#\n",
    "def get_x_tilde(X): return np.concatenate((np.ones((X.shape[ 0 ], 1 )), X), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Function that replaces initial input features by evaluating Gaussian basis functions\n",
    "# on a grid of points\n",
    "#\n",
    "# Inputs:\n",
    "#\n",
    "# l: hyper-parameter for the width of the Gaussian basis functions\n",
    "# Z: location of the Gaussian basis functions\n",
    "# X: points at which to evaluate the basis functions\n",
    "#\n",
    "# Output: Feature matrix with the evaluations of the Gaussian basis functions.\n",
    "#\n",
    "\n",
    "def evaluate_basis_functions(l, X, Z):\n",
    "    X2 = np.sum(X**2, 1)\n",
    "    Z2 = np.sum(Z**2, 1)\n",
    "    ones_Z = np.ones(Z.shape[ 0 ])\n",
    "    ones_X = np.ones(X.shape[ 0 ])\n",
    "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
    "    return np.exp(-0.5 / l**2 * r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Function that makes predictions with a logistic classifier\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X_tile: matrix of input features (with a constant 1 appended to the left) \n",
    "#         for which to make predictions\n",
    "# w: vector of model parameters\n",
    "#\n",
    "# Output: The predictions of the logistic classifier\n",
    "#\n",
    "\n",
    "def predict(X_tilde, w): return logistic(np.dot(X_tilde, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code implementation of Bayesian logistic regression with Laplace approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Computes negative log of approximated Gaussian posterior\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# w: Vector of model parameters w\n",
    "# X_tilde: 2d array with input features (augmented)\n",
    "# y: 1d array with output class labels (0 or 1)\n",
    "# S: Covariance matrix of Gaussian prior\n",
    "# m0: mean of Gaussian prior\n",
    "#\n",
    "# Output: Vector of negative log of approximated Gaussian posterior\n",
    "#\n",
    "def neg_log_posterior(w, X_tilde, y, S, m0):\n",
    "    k = predict(X_tilde, w)\n",
    "    log_prior = -0.5 * np.dot(np.dot(w-m0, np.linalg.inv(S)), w-m0)\n",
    "    ll = np.sum(y * np.log(k) + (1 - y) * np.log(1-k))\n",
    "\n",
    "    return -(ll + log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes negative gradient of approximated Gaussian posterior, needed for L_BFGS_B optimisation\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# w: Vector of model parameters w\n",
    "# X_tilde: 2d array with input features (augmented)\n",
    "# y: 1d array with output class labels (0 or 1)\n",
    "# S: Covariance matrix of Gaussian prior\n",
    "# m0: mean of Gaussian prior\n",
    "# Output: Vector of negative gradient of approximated Gaussian posterior\n",
    "\n",
    "def grad_neg_log_posterior(w, X_tilde, y, S, m0):\n",
    "    k = predict(X_tilde, w)\n",
    "    log_prior_grad = - np.dot(np.linalg.inv(S), w) + np.dot(np.linalg.inv(S), m_0)\n",
    "    ll_grad = X_tilde.T @ (y - k)\n",
    "    \n",
    "    return -(log_prior_grad + ll_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes Hessian matrix (not inverse) of likelihood, needed for defining the Gaussian approximation\n",
    "#\n",
    "# Input:\n",
    "# w: Vector of model parameters w\n",
    "# X_tilde: 2d array with input features (augmented)\n",
    "# y: 1d array with output class labels (0 or 1)\n",
    "# S: Covariance matrix of Gaussian prior\n",
    "# m0: mean of Gaussian prior\n",
    "#\n",
    "# Output: Hessian matrix of likelihood\n",
    "\n",
    "def hessian(w, X_tilde, y, S, m0):\n",
    "    k = predict(X_tilde, w)\n",
    "    h = np.linalg.inv(S)\n",
    "    for x in X_tilde:\n",
    "        k = predict(x, w)\n",
    "        h += np.outer(x, x) * k * (1 - k)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_w_map(X_tilde, y, S, m0):\n",
    "    w0 = np.zeros(X_tilde.shape[1])\n",
    "    w_map, _, _ = fmin_l_bfgs_b(neg_log_posterior, w0, grad_neg_log_posterior, args=(X_tilde, y, S, m0))\n",
    "    return w_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that performs Laplace approximation for Bayesian logistic regression\n",
    "#\n",
    "# Input:\n",
    "# X_tilde: 2d array with input features (augmented)\n",
    "# y: 1d array with output class labels (0 or 1)\n",
    "# S: Covariance matrix of Gaussian prior\n",
    "# m0: mean of Gaussian prior\n",
    "#\n",
    "# Output:\n",
    "# log_evidence: Log model evidence using Laplace approximation\n",
    "# w_map: MAP solution for model weights\n",
    "\n",
    "def laplace_approx(X_tilde, y, S, m0):\n",
    "    D = X_tilde.shape[1]\n",
    "    w0 = np.zeros(X_tilde_train.shape[1])\n",
    "    w_map, _, _ = fmin_l_bfgs_b(neg_log_posterior, w0, fprime=grad_neg_log_posterior, args=(X_tilde, y, S, m0))\n",
    "\n",
    "    H = hessian(w_map, X_tilde, y, S, m0)\n",
    "    S_N = np.linalg.inv(H) # Covariance matrix of Laplace approximation\n",
    "\n",
    "    # Compute model evidence using approximation\n",
    "    log_det_H = np.linalg.slogdet(H)[1]\n",
    "    log_posterior = -neg_log_posterior(w_map, X_tilde, y, S, m0)\n",
    "    log_evidence = log_posterior - 0.5 * log_det_H - (D / 2) * np.log(2 * np.pi)\n",
    "\n",
    "    return w_map, S_N, log_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Computes the kappa value for a given sigma\n",
    "#\n",
    "# Input:\n",
    "# var: Variance\n",
    "#\n",
    "# Output: kappa value\n",
    "\n",
    "def kappa(var):\n",
    "    return (1 + np.pi * var / 8)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Bayesian classifier using Laplace approximation\n",
    "#\n",
    "# Input:\n",
    "# X_train: 2d array with training input features (augmented)\n",
    "# X_test: 2d array with test input features (augmented)\n",
    "# y_train: 1d array with training output class labels (0 or 1)\n",
    "# S: Covariance matrix of Gaussian prior\n",
    "# m0: mean of Gaussian prior\n",
    "#\n",
    "# Output:\n",
    "# Predicted probabilities for the test set\n",
    "# log_evidence: Log model evidence using Laplace approximation\n",
    "\n",
    "def bayesian_classifier(X_train, X_test, y_train, S, m0):\n",
    "    w_map, S_N, log_evidence = laplace_approx(X_train, y_train, S, m0)\n",
    "    pred_mean = X_test @ w_map\n",
    "    pred_var = np.diag(X_test @ S_N @ X_test.T)\n",
    "    return logistic(pred_mean / kappa(pred_var)), log_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the Bayesian classifier above defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# RBF\n",
    "l=0.1 # Set RBF lengthscale\n",
    "X_tilde_train = get_x_tilde(evaluate_basis_functions(l, X_train, X_train))\n",
    "X_tilde_test = get_x_tilde(evaluate_basis_functions(l, X_test, X_train))\n",
    "\n",
    "# Set prior parameters\n",
    "m_0 = np.zeros(X_tilde_train.shape[1])\n",
    "sigma_0 = 1\n",
    "S = sigma_0 * np.eye(X_tilde_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Bayesian classifier\n",
    "pred, log_evidence = bayesian_classifier(X_tilde_train, X_tilde_test, y_train, S, m_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
