%% LyX 2.2.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{textcomp}
\usepackage{graphicx}
\graphicspath{{figures_ftr/}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\makeatother

\usepackage{babel}
\begin{document}

\title{3F8: Inference\\
 Full Technical Report}

\author{Author's Name}
\maketitle
\begin{abstract}
This is the abstract. 

Try for 1-2 sentences on each of: motive (what it\textquoteright s
about), method (what was

done), key results and conclusions (the main outcomes).

\textbullet{} Don\textquoteright t exceed 3 sentences on any one.

\textbullet{} Write this last, when you know what it should say!
\end{abstract}

\section{Introduction}
\begin{enumerate}
\item What is the problem and why is it interesting?
\item What novel follow-up will the rest of your report present?
\end{enumerate}

\section{Exercise a)}
The normalisation of posterior probabilities is straightforward for conjugate Gaussian pairs, where the model evidence $Z=p(y|X)=\int p(y|X, \beta)p(\beta)d\beta$ can be directly calculated.
However, non-conjugate distributions pose a significant challenge as the posterior shape is not predefined. 
Laplace approximation resolves this issue by fitting a Gaussian $q(\beta)$ at the posterior's local maximum, efficiently approximating the model evidence Z needed for Bayesian logistic regression.

\subsection{Bayesian logistic regression}
The posterior distribution of model weights $\beta$ given the data $y$ and $X$ is defined by Bayes' Theorem as:
\begin{equation*}
    p(\beta|y, X) = \frac{p(y|X, \beta)p(\beta)}{p(y|X)}
\end{equation*}
where $p(y|X, \beta)$ is the likelihood, $p(\beta)$ is the prior distribution, and $p(y|X)$ is the model evidence or marginal likelihood.
For model simplicity, the prior is assumed to be Gaussian with zero mean and variance $\sigma_0^2$, and the likelihood is chosen as Bernoulli with the logistic sigmoid function $\sigma(\beta^T\phi)$.
Denoting $\mathbf{S}_0=\sigma_0^2\mathbf{I}$, the Gaussian prior is formalised as $p(\beta)=\mathcal{N}(\beta|0, \mathbf{S}_0)$ to unify matrix shape and expressions.

\subsection{Laplace approximation of the posterior distribution $p(\beta|y, X)$}
The expression of the Laplace approximation $q(\beta)$ can be found using the truncated Taylor expansion. Around the mode $\beta_0$ where $\frac{df(\beta)}{d\beta}\Big|_{\beta=\beta_0}=0$, monoticity of the logarithm function gives $\nabla \log f(\beta)=0$, hence:
\begin{equation*}
    \log f(\beta) \simeq \log f(\beta_0) - \frac{1}{2} \mathbf{A} (\beta - \beta_0)^2, \quad \mathbf{A}=-\nabla^2\log f(\beta)\big|_{\beta=\beta_0}
\end{equation*}

Restoring to exponential form yields a Gaussian distribution centered at $\beta_0$, which is identified using Maximum A Posteriori (MAP) estimation. With $\beta_0 = \beta_{MAP}$, the covariance of this Gaussian is defined by the Hessian matrix $\mathbf{S}_N^{-1}=-\nabla^2 \log f(\mathbf{\beta})\big|_{\mathbf{\beta}=\mathbf{\beta}_{MAP}}$. Standard Gaussian normalization method is then applied to yield the posterior approximation:
\begin{equation}\label{posterior_approximation}
    p(\beta|y, X) \approx q(\beta) = \mathcal{N}(\mathbf{\beta}|\mathbf{\beta}_0, \mathbf{S}_N^{-1}) = \frac{1}{(2\pi)^{\frac{N}{2}} \det \mathbf{S}_N^{-\frac{1}{2}}} \exp\{\frac{1}{2} (\beta - \beta_{MAP})^T \mathbf{S}_N^{-1}(\beta - \beta_{MAP})\}.
\end{equation}
Using this expression, the Hessian matrix can be simplified as:
\begin{equation}
    \mathbf{S}_N^{-1} = -\nabla^2 \log p(\beta|y, X) = \mathbf{S}_0 + \sum_{n=1}^{N} \sigma(\beta^Tx_n)(1-\sigma(\beta^T x_n))x_n x_n^T
\end{equation}

\subsection{Approximated log model evidence}
Using the approximated posterior distribution, the log model evidence can then be calculated as:
\begin{align*}
    \log p(y|X) &= \log \int p(y|X, \beta)p(\beta)d\beta \\
    &\approx \log p(y|X, \beta_{MAP}) + \log p(\beta_{MAP})- \frac{1}{2} \log |\mathbf{S}_N^{-1}| + \frac{N}{2} \log 2\pi
\end{align*}
which is a combination of the log prior, log likelihood, and the negative log of the determinant of the inverse covariance matrix for the posterior.

\subsection{Approximated predictive distribution}
For the binary classification problem concerned in this exercise, the predictive distribution is then obtained by marginilising the approximated posterior probability obtained in Equation~\ref{posterior_approximation}. For category $\mathcal{C}_1$, given a new feature vector $\phi(x)$:
\begin{equation}\label{predictive distribution}
    p(\mathcal{C}_1|\phi, y, X) = \int p(\mathcal{C}_1|\phi, \beta)p(\beta|y, X)d\beta 
    \simeq \int \sigma(\beta^T \phi)q(\beta)d\beta 
\end{equation}

Simplifying Equation~\ref{predictive distribution} using the sifting property of Dirac delta function:
\begin{equation*}
    p(\mathcal{C}_1|\phi, y, X) = \int \sigma(\beta^T\phi)\mathcal{N}(\beta^T\phi|\beta_{MAP}^T\phi, \phi^T \mathbf{S}_N\phi) = \int \sigma(\beta ^T \phi)\mathcal{N}(\mu_{pred},\sigma_{pred}^2)d\beta
\end{equation*}

so $\mu_{pred}=\beta_{MAP} ^T \phi$, $\sigma_{pred}^2=\phi^T \mathbf{S}_N \phi$. This can be further simplified by approximating the logistic sigmoid with prodit function $\Phi(\lambda x)$ with the scale factor $\lambda ^2 = \pi / 8$:
\begin{equation}
    p(\mathcal{C}_1|\phi, y, X) = \sigma(\kappa(\phi^T S_N\phi)\beta^T\phi|\beta_{MAP}^T\phi), \quad \kappa(\sigma ^2) = (1+\pi \sigma^2 / 8)^{-1/2}
\end{equation}
\section{Exercise b)}

{[} Describe the new gradient form, the python code and any specific
implementation details {]}
\begin{verbatim}
#
# Python code to be included
#
\end{verbatim}

\section{Exercise c)}

{[} Include plots in Figure \ref{fig:predictive_distributions} and
describe how the results differ from each other {]}

\begin{figure}
\begin{centering}
% \includegraphics[width=0.3\paperwidth]{place_holder_figure}\hspace{1cm}\includegraphics[width=0.3\paperwidth]{place_holder_figure}
\par\end{centering}
\caption{Plots showing data and contour lines for the predictive distribution
generated by the Laplace approximation (left) and the MAP solution
(right).\label{fig:predictive_distributions} }
\end{figure}


\section{Exercise d)}

{[} Include results in Tables \ref{tab:ll_MAP}, \ref{tab:ll_Laplace},
\ref{tab:conf_MAP} and \ref{tab:conf_Laplace} and explain the results
obtained and any findings {]}

\begin{table}
\centering{}%
\begin{minipage}[t]{0.5\textwidth}%
\begin{center}
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
$-0.220$ & $-0.293$\tabularnewline
\hline 
\end{tabular}\caption{Log-likelihoods for MAP solution.\label{tab:ll_MAP}}
\par\end{center}%
\end{minipage}%
\begin{minipage}[t]{0.5\textwidth}%
\begin{center}
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
$-0.260$ & $-0.318$\tabularnewline
\hline 
\end{tabular}\caption{Log-likelihoods for Laplace approximation.\label{tab:ll_Laplace}}
\par\end{center}%
\end{minipage}
\end{table}

\begin{table}
\centering{}%
\begin{minipage}[t]{0.5\textwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & $0.949$ & $0.051$\tabularnewline
\cline{2-4} 
 & 1 & $0.059$ & $0.941$\tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Conf. matrix for for MAP solution.\label{tab:conf_MAP}}
%
\end{minipage}%
\begin{minipage}[t]{0.5\textwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & $0.949$ & $0.051$\tabularnewline
\cline{2-4} 
 & 1 & $0.059$ & $0.941$\tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Conf. matrix for Laplace approximation.\label{tab:conf_Laplace}}
%
\end{minipage}
\end{table}


\section{Exercise e)}

{[} describe your grid search approach, the python code, the grid
points chosen, the heat map plot from Figure \ref{fig:heat_map_plot}
and the best hyper-parameter values obtained via grid search {]}
\begin{verbatim}
#
# Python code to be included
#
\end{verbatim}
\begin{figure}
\begin{centering}
% \includegraphics[width=0.3\paperwidth]{place_holder_figure}
\par\end{centering}
\caption{Heat map plot of the the approximation of the model evidence obtained
in the grid search.\label{fig:heat_map_plot} }
\end{figure}


\section{Exercise f)}

{[} Describe the visualisation of the predictions in Figure \ref{fig:prediction_visualisation_after_tuning}
and the results in Tables \ref{tab:average_ll_after_tuning} and \ref{tab:confusion_after_tuning}.
How do they compare to the ones obtained in previous exercises? {]}

\begin{figure}
\begin{centering}
% \includegraphics[width=0.3\paperwidth]{place_holder_figure}
\par\end{centering}
\caption{Visualisation of the contours of the class predictive probabilities
for Laplace approximation after hyper-parameter tuning by maximising
the model evidence.\label{fig:prediction_visualisation_after_tuning} }
\end{figure}

\begin{table}
\centering{}%
\begin{minipage}[t]{0.4\columnwidth}%
\begin{center}
\vspace{-0.2cm}%
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
- & -\tabularnewline
\hline 
\end{tabular} 
\par\end{center}
\caption{Average training and test log-likelihoods for Laplace approximation
after hyper-parameter tuning by maximising the model evidence.\label{tab:average_ll_after_tuning}}
%
\end{minipage}\hspace{2cm}%
\begin{minipage}[t]{0.4\columnwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & - & -\tabularnewline
\cline{2-4} 
 & 1 & - & -\tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Confusion matrix for Laplace approximation after hyper-parameter tuning
by maximising the model evidence.\label{tab:confusion_after_tuning}}
%
\end{minipage}
\end{table}


\section{Conclusions}
\begin{enumerate}
\item Draw together the most important results and their consequences.
\item List any reservations or limitations.
\end{enumerate}

\end{document}
